import pandas as pd
import os
import jieba
from gensim import corpora, models
from gensim.models import CoherenceModel
from snownlp import SnowNLP
from tqdm import tqdm
import matplotlib.pyplot as plt
from matplotlib import rcParams, font_manager
import numpy as np
from multiprocessing import freeze_support
import pyLDAvis.gensim_models as gensimvis # å¯¼å…¥ pyLDAvis çš„ gensim æ¨¡å—
import pyLDAvis # å¯¼å…¥ pyLDAvis

# å¯ç”¨tqdmçš„pandasé›†æˆ
tqdm.pandas()

# --- å¯ç¼–è¾‘é…ç½® ---
CONFIG = {
    # è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆéœ€åŒ…å«åˆ— content æˆ–ä»¥ content å¼€å¤´çš„å¤šåˆ—ï¼‰
    'input_csv_path': '/Users/ziming_ye/Python/BERTopic/å¼€ç›’è¯„è®ºé›†åˆï¼ˆ6å¹³å°ï¼‰.csv',
    # è¾“å‡ºæ ¹ç›®å½•ï¼ˆå¯é€‰ï¼‰ã€‚è‹¥ä¸ºNoneï¼Œåˆ™åœ¨è¾“å…¥CSVåŒç›®å½•ä¸‹åˆ›å»ºåŒåæ–‡ä»¶å¤¹
    'output_root_dir': None,
}

# --- å…¨å±€å‡½æ•°å’Œé…ç½® ---

# åŠ è½½åœç”¨è¯
def load_stopwords(filepath='/Volumes/ZimingYe/Python/cn_all_stopwords.txt'):
    """åŠ è½½åœç”¨è¯"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return set([line.strip() for line in f])

# å…¨å±€åœç”¨è¯å˜é‡ï¼ŒåªåŠ è½½ä¸€æ¬¡
stopwords = load_stopwords()

# ä¸­æ–‡åˆ†è¯å‡½æ•°
def tokenize(text):
    """åˆ†è¯ï¼Œå¹¶å»é™¤åœç”¨è¯å’Œå•ä¸ªå­—ç¬¦"""
    return [w for w in jieba.lcut(text) if w not in stopwords and len(w.strip()) > 1]

# è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
def sentiment_score(text):
    """è®¡ç®—æƒ…æ„Ÿå¾—åˆ†ï¼ˆ0åˆ°1ä¹‹é—´ï¼Œè¶Šæ¥è¿‘1è¶Šç§¯æï¼‰"""
    try:
        return SnowNLP(text).sentiments
    except:
        return 0.5 # å¼‚å¸¸æƒ…å†µè¿”å›ä¸­æ€§

# æ‰©å±•æƒ…æ„Ÿåˆ†æï¼ˆåˆ†ç±»ä¸ºç§¯æã€ä¸­æ€§ã€æ¶ˆæï¼‰
def extended_sentiment_analysis(text):
    """æ ¹æ®æƒ…æ„Ÿå¾—åˆ†åˆ†ç±»ä¸º 'ç§¯æ', 'ä¸­æ€§', 'æ¶ˆæ'"""
    try:
        score = SnowNLP(text).sentiments
        if score > 0.7:
            return "ç§¯æ"
        elif score < 0.3:
            return "æ¶ˆæ"
        else:
            return "ä¸­æ€§"
    except:
        return "æœªçŸ¥"

# --- ä¸»æµç¨‹å‡½æ•° ---

def prepare_output_dir(input_csv_path, output_dir=None):
    """åŸºäºè¾“å…¥CSVåˆ›å»ºè¾“å‡ºç›®å½•ï¼Œè‹¥æŒ‡å®šäº†output_diråˆ™ä½¿ç”¨æŒ‡å®šç›®å½•"""
    if output_dir:
        target_dir = output_dir
    else:
        # ç›´æ¥åœ¨è¾“å…¥CSVçš„åŒä¸€æ–‡ä»¶å¤¹å†…åˆ›å»ºåŒåå­æ–‡ä»¶å¤¹
        base_name = os.path.splitext(os.path.basename(input_csv_path))[0]
        parent_dir = os.path.dirname(input_csv_path)
        target_dir = os.path.join(parent_dir, base_name)
    
    try:
        os.makedirs(target_dir, exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {target_dir}")
        
        # éªŒè¯ç›®å½•æ˜¯å¦çœŸçš„åˆ›å»ºæˆåŠŸ
        if os.path.exists(target_dir) and os.path.isdir(target_dir):
            print(f"âœ… è¾“å‡ºç›®å½•åˆ›å»ºæˆåŠŸ")
        else:
            print(f"âŒ è¾“å‡ºç›®å½•åˆ›å»ºå¤±è´¥: {target_dir}")
            # å°è¯•åˆ›å»ºçˆ¶ç›®å½•
            parent = os.path.dirname(target_dir)
            if not os.path.exists(parent):
                os.makedirs(parent, exist_ok=True)
                os.makedirs(target_dir, exist_ok=True)
                print(f"âœ… é‡æ–°åˆ›å»ºè¾“å‡ºç›®å½•æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºè¾“å‡ºç›®å½•æ—¶å‡ºé”™: {e}")
        # å›é€€åˆ°å½“å‰å·¥ä½œç›®å½•
        target_dir = os.path.join(os.getcwd(), "LDA_è¾“å‡ºç»“æœ")
        os.makedirs(target_dir, exist_ok=True)
        print(f"ğŸ“ ä½¿ç”¨å›é€€ç›®å½•: {target_dir}")
    
    return target_dir


def main():
    input_csv_path = CONFIG['input_csv_path']
    output_dir = CONFIG.get('output_root_dir')
    # 1. è¯»å–æ•°æ®
    print("\nğŸ“š æ­£åœ¨è¯»å–æ•°æ®...")
    df = pd.read_csv(input_csv_path)

    # è‡ªåŠ¨æ£€æµ‹å¹¶åˆå¹¶æ‰€æœ‰ä»¥ content å¼€å¤´çš„åˆ—
    content_like_columns = [col for col in df.columns if str(col).lower().startswith('content')]
    if len(content_like_columns) == 0:
        raise ValueError("æœªåœ¨è¾“å…¥CSVä¸­æ‰¾åˆ°ä»¥ 'content' å¼€å¤´çš„åˆ—ï¼Œè¯·æ£€æŸ¥æ•°æ®åˆ—åã€‚")

    print(f"ğŸ” æ£€æµ‹åˆ°ç”¨äºåˆ†æçš„åˆ—: {content_like_columns}")

    def merge_content_columns(row):
        parts = []
        for column_name in content_like_columns:
            value = row.get(column_name)
            if pd.notna(value):
                parts.append(str(value))
        return 'ã€‚'.join(parts).strip()

    df['content'] = df.apply(merge_content_columns, axis=1)
    df = df[df['content'].astype(str).str.strip() != '']  # å»é™¤ç©ºè¯„è®º
    texts = df['content'].astype(str).tolist()

    # 2. ä¸­æ–‡åˆ†è¯
    print("âœ‚ï¸ æ­£åœ¨è¿›è¡Œåˆ†è¯...")
    tokenized_texts = [tokenize(text) for text in tqdm(texts, desc="åˆ†è¯ä¸­")]

    # 3. æ„å»ºå­—å…¸å’Œè¯­æ–™åº“
    print("ğŸ“– æ„å»ºå­—å…¸å’Œè¯­æ–™åº“...")
    dictionary = corpora.Dictionary(tokenized_texts)
    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]


    # 4. æ„å»ºLDAæ¨¡å‹ï¼ˆå›ºå®š6ä¸ªä¸»é¢˜ï¼‰
    print("\nğŸ” æ­£åœ¨è®­ç»ƒLDAæ¨¡å‹ï¼ˆå›ºå®š6ä¸ªä¸»é¢˜ï¼‰...")
    
    # å›ºå®šä¸»é¢˜æ•°ç›®ä¸º6
    best_num_topics = 6
    print(f"ğŸ¯ ä½¿ç”¨å›ºå®šä¸»é¢˜æ•°ç›®: {best_num_topics}")
    
    # æ„å»ºLDAæ¨¡å‹
    lda_model = models.LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=best_num_topics,
        passes=10,
        random_state=42
    )
    print("âœ… LDAæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    # è®¡ç®—å¹¶æ˜¾ç¤ºæ¨¡å‹è¯„ä¼°æŒ‡æ ‡
    coherence_model = CoherenceModel(
        model=lda_model,
        texts=tokenized_texts,
        dictionary=dictionary,
        coherence='c_v'
    )
    coherence_score = coherence_model.get_coherence()
    perplexity_score = lda_model.log_perplexity(corpus)
    
    print(f"ğŸ“Š æ¨¡å‹è¯„ä¼°æŒ‡æ ‡:")
    print(f"  ä¸€è‡´æ€§ c_v: {coherence_score:.4f}")
    print(f"  å›°æƒ‘åº¦: {perplexity_score:.2f}")
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_base_dir = prepare_output_dir(input_csv_path, output_dir)

    print("\nğŸ§  LDAä¸»é¢˜å…³é”®è¯å±•ç¤ºï¼š")
    for i, topic in lda_model.show_topics(num_words=10, formatted=True):
        print(f"ä¸»é¢˜ {i}: {topic}")

    # 5. æƒ…æ„Ÿåˆ†æ
    print("\nğŸ’­ æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ...")
    df['sentiment'] = df['content'].progress_apply(sentiment_score)
    df['sentiment_category'] = df['content'].progress_apply(extended_sentiment_analysis)

    # 6. æ¯æ¡è¯„è®ºå½’ç±»åˆ°ä¸»ä¸»é¢˜
    print("\nğŸ“Š æ­£åœ¨è¿›è¡Œä¸»é¢˜åˆ†ç±»...")
    def get_main_topic(text_content):
        # ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„tokenizeå‡½æ•°
        tokens = tokenize(text_content)
        bow = dictionary.doc2bow(tokens)
        topics = lda_model.get_document_topics(bow)
        # è¿”å›æ¦‚ç‡æœ€å¤§çš„ä¸»é¢˜ç¼–å·
        return max(topics, key=lambda x: x[1])[0] if topics else -1

    df['topic'] = df['content'].progress_apply(get_main_topic)

    # ç¡®ä¿æƒ…æ„Ÿä¸»é¢˜æ•°é‡ä¸ LDA ä¸»é¢˜æ•°é‡ä¸€è‡´ (è¿‡æ»¤æœªå½’ç±»çš„è¯„è®º)
    original_comments = len(df)
    df = df[df['topic'] != -1]
    print(f"ğŸ” è¿‡æ»¤æ‰æœªå½’ç±»ä¸»é¢˜çš„è¯„è®ºï¼Œå‰©ä½™è¯„è®ºæ•°é‡: {len(df)} (åŸ:{original_comments})")

    # 7. æŒ‰ä¸»é¢˜æ±‡æ€»æƒ…æ„Ÿå¾—åˆ†
    summary = df.groupby('topic')['sentiment'].agg(['mean', 'count']).reset_index()
    summary.columns = ['ä¸»é¢˜', 'å¹³å‡æƒ…æ„Ÿ', 'è¯„è®ºæ•°é‡']
    print("\nğŸ“Š æ¯ä¸ªä¸»é¢˜çš„å¹³å‡æƒ…æ„Ÿå¾—åˆ†ï¼š")
    print(summary)

    # 8. å¯è§†åŒ–æƒ…æ„Ÿåˆ†å¸ƒ (ä¿æŒåŸæ ·)
    print("\nğŸ” æ­£åœ¨è®¡ç®—æƒ…æ„Ÿåˆ†å¸ƒå¹¶å¯è§†åŒ–...")
    sentiment_distribution = df.groupby(['topic', 'sentiment_category']).size().reset_index(name='count')
    print("\nğŸ“Š æ¯ä¸ªä¸»é¢˜çš„æƒ…æ„Ÿç±»åˆ«åˆ†å¸ƒï¼š")
    print(sentiment_distribution)

    # è®¾ç½®ä¸­æ–‡å­—ä½“
    try:
        # å°è¯•å¤šç§ä¸­æ–‡å­—ä½“è·¯å¾„
        font_paths = [
            '/Users/ziming_ye/Python/Simhei.ttf',
            '/System/Library/Fonts/PingFang.ttc',
            '/System/Library/Fonts/STHeiti Light.ttc',
            '/System/Library/Fonts/STHeiti Medium.ttc'
        ]
        
        font_set = False
        for font_path in font_paths:
            if os.path.exists(font_path):
                try:
                    font_manager.fontManager.addfont(font_path)
                    rcParams['font.sans-serif'] = ['SimHei', 'PingFang SC', 'STHeiti', 'Arial Unicode MS']
                    font_set = True
                    print(f"âœ… ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font_path}")
                    break
                except:
                    continue
        
        if not font_set:
            # ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸­æ–‡å­—ä½“
            rcParams['font.sans-serif'] = ['PingFang SC', 'STHeiti', 'Arial Unicode MS', 'SimHei']
            print("âš ï¸ ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸­æ–‡å­—ä½“")
            
    except Exception as e:
        # å›é€€åˆ°ç³»ç»Ÿé»˜è®¤ä¸­æ–‡å­—ä½“
        rcParams['font.sans-serif'] = ['PingFang SC', 'STHeiti', 'Arial Unicode MS', 'SimHei']
        print(f"âš ï¸ å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“: {e}")
    
    rcParams['axes.unicode_minus'] = False

    # å­¦æœ¯è®ºæ–‡å‹å¥½ä¸”è‰²ç›²å®‰å…¨çš„ Okabeâ€“Ito é…è‰²
    # ç§¯æ: ç»¿è‰²  ä¸­æ€§: ç°è‰²  æ¶ˆæ: æœ±çº¢
    morandi_colors = {
        'ç§¯æ': '#009E73',
        'ä¸­æ€§': '#7F7F7F',
        'æ¶ˆæ': '#D55E00'
    }

    # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„æƒ…æ„Ÿåˆ†å¸ƒæ¯”ä¾‹
    topic_sentiments = {}
    for topic in sorted(sentiment_distribution['topic'].unique()): # ç¡®ä¿ä¸»é¢˜é¡ºåº
        topic_data = sentiment_distribution[sentiment_distribution['topic'] == topic]
        total = topic_data['count'].sum()
        proportions = {}
        for sentiment in ['ç§¯æ', 'ä¸­æ€§', 'æ¶ˆæ']:
            count = topic_data[topic_data['sentiment_category'] == sentiment]['count'].values
            proportions[sentiment] = count[0] / total if len(count) > 0 else 0
        topic_sentiments[topic] = proportions

    # ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
    plt.figure(figsize=(12, 7))
    x = list(topic_sentiments.keys())
    bottom = np.zeros(len(x))

    for sentiment in ['ç§¯æ', 'ä¸­æ€§', 'æ¶ˆæ']:
        proportions = [topic_sentiments[topic][sentiment] for topic in x]
        plt.bar(x, proportions, bottom=bottom,
                label=sentiment,
                color=morandi_colors[sentiment],
                alpha=0.85,
                width=0.6)
        bottom += proportions

    # è®¾ç½®å›¾è¡¨æ ·å¼
    plt.xlabel('ä¸»é¢˜ç¼–å·', fontsize=11)
    plt.ylabel('æ¯”ä¾‹', fontsize=11)
    plt.title('å„ä¸»é¢˜çš„æƒ…æ„Ÿç±»åˆ«æ¯”ä¾‹åˆ†å¸ƒ', fontsize=13, pad=15)

    # è‡ªå®šä¹‰å›¾ä¾‹æ ·å¼
    legend = plt.legend(title='æƒ…æ„Ÿç±»åˆ«', title_fontsize=11, fontsize=10,
                        bbox_to_anchor=(1.02, 1), loc='upper left')
    legend.get_frame().set_alpha(0.9)
    legend.get_frame().set_edgecolor('#E5E5E5')

    # è®¾ç½®ç½‘æ ¼çº¿å’Œåˆ»åº¦
    plt.grid(True, linestyle='--', alpha=0.3, color='#666666')
    plt.ylim(0, 1.0)
    plt.yticks([i/10 for i in range(0, 11)], [f'{i*10}%' for i in range(0, 11)])

    # è®¾ç½®èƒŒæ™¯è‰²
    plt.gca().set_facecolor('#FAFAFA')
    plt.gcf().patch.set_facecolor('#FAFAFA')

    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    plt.subplots_adjust(right=0.85)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i in x:
        total = 0
        for sentiment in ['ç§¯æ', 'ä¸­æ€§', 'æ¶ˆæ']:
            value = topic_sentiments[i][sentiment]
            if value > 0.05:
                plt.text(i, total + value/2, f'{value:.0%}',
                        ha='center', va='center',
                        color='white', fontsize=9, fontweight='bold')
            total += value

    # ä¿å­˜å›¾è¡¨
    stacked_png_path = os.path.join(output_base_dir, 'ä¸»é¢˜æƒ…æ„Ÿå †å åˆ†å¸ƒ.png')
    plt.savefig(stacked_png_path,
                dpi=300,
                bbox_inches='tight',
                facecolor='#FAFAFA')
    plt.show()
    print(f"âœ… ä¸»é¢˜æƒ…æ„Ÿå †å åˆ†å¸ƒå›¾å·²ä¿å­˜: {stacked_png_path}")

    # 9. ç”Ÿæˆäº¤äº’å¼LDAå›¾
    print("\nğŸ” æ­£åœ¨ç”Ÿæˆ LDA çš„ HTML å›¾...")
    # pyLDAvis.enable_notebook() # Only if running in a Jupyter Notebook
    lda_vis = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)
    html_path = os.path.join(output_base_dir, 'lda_topics_visualization.html')
    pyLDAvis.save_html(lda_vis, html_path)
    print(f"âœ… LDA çš„ HTML å›¾å·²ä¿å­˜ä¸º: {html_path}")

    # 10. æ•°æ®ä¿å­˜
    result_csv_path = os.path.join(output_base_dir, 'æƒ…æ„Ÿä¸»é¢˜åˆ†ææ‰©å±•ç»“æœ.csv')
    df.to_csv(result_csv_path, index=False, encoding='utf-8-sig')
    print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {result_csv_path}")

    # 11. ç”Ÿæˆå•ç‹¬çš„æƒ…æ„Ÿåˆ†ç±»è¡¨ (åŸºäºè¿‡æ»¤åçš„æ•°æ®)
    print("\nğŸ” æ­£åœ¨ç”Ÿæˆå•ç‹¬çš„æƒ…æ„Ÿåˆ†ç±»è¡¨...")
    standalone_sentiment_distribution = df.groupby(['sentiment_category']).size().reset_index(name='count')
    print("\nğŸ“Š å•ç‹¬çš„æƒ…æ„Ÿåˆ†ç±»åˆ†å¸ƒï¼š")
    print(standalone_sentiment_distribution)

    # ä¿å­˜å•ç‹¬çš„æƒ…æ„Ÿåˆ†ç±»è¡¨
    standalone_csv_path = os.path.join(output_base_dir, 'å•ç‹¬æƒ…æ„Ÿåˆ†ç±»ç»“æœ.csv')
    standalone_sentiment_distribution.to_csv(standalone_csv_path, index=False, encoding='utf-8-sig')
    print(f"âœ… å•ç‹¬æƒ…æ„Ÿåˆ†ç±»ç»“æœå·²ä¿å­˜: {standalone_csv_path}")

    # 12. æŒ‰ä¸»é¢˜ä¿å­˜è¯„è®ºæ–‡æœ¬
    print("\nğŸ” æ­£åœ¨æŒ‰ä¸»é¢˜ä¿å­˜è¯„è®ºæ–‡æœ¬...")
    for topic_id in sorted(df['topic'].unique()):
        topic_comments = df[df['topic'] == topic_id][['content', 'sentiment', 'sentiment_category']].copy()
        topic_csv_path = os.path.join(output_base_dir, f'ä¸»é¢˜{topic_id}_è¯„è®ºæ–‡æœ¬.csv')
        topic_comments.to_csv(topic_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ä¸»é¢˜{topic_id}çš„{len(topic_comments)}æ¡è¯„è®ºå·²ä¿å­˜: {topic_csv_path}")
    
    # 13. ç”Ÿæˆä¸»é¢˜æ±‡æ€»è¡¨ï¼ˆåŒ…å«æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯å’Œè¯„è®ºæ•°é‡ï¼‰
    print("\nğŸ” æ­£åœ¨ç”Ÿæˆä¸»é¢˜æ±‡æ€»è¡¨...")
    topic_summary = []
    for topic_id in sorted(df['topic'].unique()):
        topic_comments = df[df['topic'] == topic_id]
        topic_words = lda_model.show_topics(num_topics=best_num_topics, formatted=False)[topic_id]
        top_words = ', '.join([word for word, prob in topic_words[1][:5]])  # å‰5ä¸ªå…³é”®è¯
        
        topic_summary.append({
            'ä¸»é¢˜ç¼–å·': topic_id,
            'å…³é”®è¯': top_words,
            'è¯„è®ºæ•°é‡': len(topic_comments),
            'å¹³å‡æƒ…æ„Ÿå¾—åˆ†': topic_comments['sentiment'].mean(),
            'ç§¯æè¯„è®ºæ•°': len(topic_comments[topic_comments['sentiment_category'] == 'ç§¯æ']),
            'ä¸­æ€§è¯„è®ºæ•°': len(topic_comments[topic_comments['sentiment_category'] == 'ä¸­æ€§']),
            'æ¶ˆæè¯„è®ºæ•°': len(topic_comments[topic_comments['sentiment_category'] == 'æ¶ˆæ'])
        })
    
    topic_summary_df = pd.DataFrame(topic_summary)
    topic_summary_path = os.path.join(output_base_dir, 'ä¸»é¢˜æ±‡æ€»è¡¨.csv')
    topic_summary_df.to_csv(topic_summary_path, index=False, encoding='utf-8-sig')
    print(f"âœ… ä¸»é¢˜æ±‡æ€»è¡¨å·²ä¿å­˜: {topic_summary_path}")
    print("\nğŸ“Š ä¸»é¢˜æ±‡æ€»è¡¨é¢„è§ˆï¼š")
    print(topic_summary_df)

if __name__ == '__main__':
    freeze_support() # ç”¨äºåœ¨Windowså¤šè¿›ç¨‹ç¯å¢ƒä¸‹é˜²æ­¢é€’å½’åˆ›å»ºè¿›ç¨‹
    main()